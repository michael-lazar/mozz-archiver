#!/usr/bin/env python3
"""
Create a searchable index for a WARC archive.
"""
import argparse
import sqlite3
import pathlib
import re
from urllib.parse import urlparse

from warcio.archiveiterator import ArchiveIterator


SQL_INITIALIZE_TABLE = """
CREATE TABLE IF NOT EXISTS requests (
    url TEXT,
    netloc TEXT,
    warc_offset INTEGER,
    warc_length INTEGER,
    warc_filename TEXT,
    response_status TEXT,
    response_meta TEXT,
    error_message TEXT
);
CREATE UNIQUE INDEX IF NOT EXISTS request_url_index ON requests (url);
"""


LOG_PATTERNS = [
    (
        "URL forbidden by block list",
        re.compile("DEBUG: Forbidden by URL deny list: <GET (?P<url>.+)>",)
    ),
    (
        "URL forbidden by robots.txt",
        re.compile(r"DEBUG: Forbidden by robots\.txt: <GET (?P<url>.+)>"),
    ),
    (
        "Download exceeded max size of 100 MB",
        re.compile("ERROR: Received ([0-9]+) bytes larger than download max size ([0-9]+) in request <GET (?P<url>.+)>"),
    ),
    (
        "Download timed out after 60 seconds",
        re.compile(r"ERROR: Getting <GET (?P<url>.+)> took longer than 60\.0 seconds")
    ),
    (
        "Error: {message}",
        re.compile("ERROR: Error downloading <GET (?P<url>.+)>: (?P<message>.+)")
    ),
]

TRACEBACK_START_PATTERN = re.compile("ERROR: Error downloading <GET (?P<url>.+)>$")
TRACEBACK_END_PATTERN = re.compile("^[A-Za-z.]+: (?P<message>.+)")


def index_request(record, iterator, filename, conn):
    url = record.rec_headers.get_header("WARC-Target-URI")
    netloc = urlparse(url).netloc
    print(url)

    header = record.content_stream().readline().decode('utf-8')
    parts = header.strip().split(maxsplit=1)
    if len(parts) == 0:
        status, meta = None, None
    elif len(parts) == 1:
        status, meta = parts[0], None
    else:
        status, meta = parts[0], parts[1]

    warc_length = iterator.get_record_length()
    warc_offset = iterator.get_record_offset()
    warc_filename = filename

    conn.execute(
        "INSERT OR REPLACE INTO requests VALUES (?,?,?,?,?,?,?,?);",
        (url, netloc, warc_offset, warc_length, warc_filename, status, meta, None),
    )


def index_error(url, error_message, conn):
    print(f"<{url}> {error_message}")

    # If we have a successful response, don't overwrite it with an error
    c = conn.cursor()
    c.execute(
        "SELECT * FROM requests WHERE url=? AND error_message IS NULL;",
        (url,)
    )
    if c.fetchone():
        return

    conn.execute(
        "INSERT OR REPLACE INTO requests(url, error_message) VALUES (?,?)",
        (url, error_message),
    )


parser = argparse.ArgumentParser()
parser.add_argument('--warc-dir')
parser.add_argument('--crawl-logfile')
parser.add_argument('--database', default="./index.sqlite")
args = parser.parse_args()

conn = sqlite3.connect(args.database, isolation_level=None)
conn.row_factory = sqlite3.Row
conn.executescript(SQL_INITIALIZE_TABLE)


if args.warc_dir:
    warc_dir = pathlib.Path(args.warc_dir).resolve()
    assert warc_dir.is_dir()

    request_url_map = {}

    files = sorted(warc_dir.glob("*.warc.gz"))
    for file in files:
        with file.open('rb') as fp:
            iterator = ArchiveIterator(fp)
            for record in iterator:
                if record.rec_type == "response":
                    index_request(record, iterator, file.name, conn)


if args.crawl_logfile:
    logfile = pathlib.Path(args.crawl_logfile).resolve()
    assert logfile.exists()

    traceback_url = None
    with logfile.open('r') as fp:
        for line in fp:
            if traceback_url:
                match = TRACEBACK_END_PATTERN.search(line)
                if match:
                    error_message = f'Error: {match.group("message")}'
                    index_error(traceback_url, error_message, conn)
                    traceback_url = None
            else:
                match = TRACEBACK_START_PATTERN.search(line)
                if match:
                    traceback_url = match.group("url")
                else:
                    for template, regex in LOG_PATTERNS:
                        match = regex.search(line)
                        if match:
                            url = match.group('url')
                            error_message = template.format(**match.groupdict())
                            index_error(url, error_message, conn)
                            break
